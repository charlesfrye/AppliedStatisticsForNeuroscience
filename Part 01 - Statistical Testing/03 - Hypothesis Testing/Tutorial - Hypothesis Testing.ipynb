{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Hypothesis Testing\n",
    "\n",
    "This tutorial covers the basis of hypothesis testing, including null hypotheses, error types, error rates, and $p$-values. These will require us to develop some fundamental ideas from probability, including joint, marginal, and conditional probabilities.\n",
    "\n",
    "If you'd like a more thorough-going introduction to these ideas, along with an introduction to Bayes' Rule for relating them to each other, check out\n",
    "[this blogpost](http://charlesfrye.github.io/stats/2016/02/04/bayes-rule.html).\n",
    "\n",
    "### Hypothesis Testing is a Kind of Decision-Making Under Uncertainty\n",
    "\n",
    "In the lab on inferential statistics, we discussed one class of inference problems: trying to infer the value of a parameter, like the average value or spread of some random variable in a population. Examples of random variables and populations include:\n",
    "\n",
    "- neural firing rate and mouse somatosensory neurons\n",
    "- whether an individual smokes or not and Canadians under the age of 25\n",
    "- the mean of the data you measured in your experiment and the collection of all datasets you could've measured\n",
    "\n",
    "We stated that the purpose of such inferences was to guide decision making. For example, we might measure the average response of a sick population to a candidate drug and then use that value to determine whether to prescribe the drug or not. We left unsaid, however, just exactly how statistical inferences are to be used.\n",
    "\n",
    "In this tutorial, we'll work through how to use statistical inferences to guide the simplest kinds of decisions: yes-or-no decisions, also known as *binary* decisions, since there are two choices. We'll focus on the yes-or-no decision of most interest to scientists: is my hypothesis true or not?\n",
    "\n",
    "### Binary Hypothesis Testing\n",
    "\n",
    "In an experimental science context, a binary hypothesis is an answer to a question that usually looks something like: \"does this intervention have an effect?\" Examples include:\n",
    "\n",
    "- Do neurons subjected to trauma express different levels of protein XYZ?\n",
    "- Does adding distractors increase reaction time in healthy human subjects?\n",
    "- Does optogenetically stimulating neural circuit A change the activity of neural circuit B?\n",
    "\n",
    "Each of these questions has a yes or no answer: \"yes, the intervention has an effect\" or \"no, the intervention has no effect\". We call these answers *hypotheses*. The hypothesis that the intervention has no effect is called the *null hypothesis*, while the hypothesis that the intervention has an effect is called the *alternative hypothesis*. These are frequently written as $H_0$ and $H_A$, with $0$ and $A$ standing for for \"null\" and \"alternative\".\n",
    "\n",
    "When we answer a binary question, there are two possible answers: yes and no, which we call the \"positive\" and \"negative\" answer. Furthermore, either the alternative or the null hypothesis could be true. Therefore, there are four possibilities, which appear in the table below:\n",
    "\n",
    "|                    |  $H_A$ is True | $H_0$ is True  |\n",
    "|:------------------:|:--------------:|----------------|\n",
    "| **We claim $H_A$** |  True Positive | False Positive |\n",
    "| **We claim $H_0$** | False Negative | True Negative  |\n",
    "\n",
    "The nomenclature for each of these four events is intuitive: the first word is \"true\" or \"false\" depending on whether out answer was correct or incorrect (not, e.g., whether the alternative hypothesis is true or false) and the second word is \"positive\" or \"negative\" depending on what we claimed.\n",
    "\n",
    "The historical names for the off-diagonal elements of this chart, which correspond to incorrect assertions or \"errors\", are \"Type I Error\" and \"Type II Error\". These names are not to be preferred, since they literally correspond to the (arbitrary) order in which the kinds of errors were described in an early paper by Neyman and Pearson. As such, they have no descriptive power, unlike \"false positive\" and \"false negative\".\n",
    "\n",
    "#### Example: Touching Cat Feet\n",
    "\n",
    "Let's work through a quick example from one of the early Nobel Prizes granted to a neuroscientist, [ED Adrian](https://www.nobelprize.org/nobel_prizes/medicine/laureates/1932/adrian-lecture.html). Take our alternative hypothesis to be \"pressing a weight onto a cat's hind foot causes increased firing in the nerve exiting the foot\". The alternative hypothesis, then, is that no such increase in firing occurs. The figures depicting the experimental apparatus from [Adrian and Yngve's 1926 paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1514868/) testing this hypothesis appear below.\n",
    "\n",
    "The simplest test we might perform to determine the veracity of this hypothesis is to first measure the firing rate of the sensory nerve when the foot is not being stimulated, then measure it again when the foot is being stimulated. We take the difference of these two values, and we accept the alternative hypothesis if the value is less than $0$. \n",
    "\n",
    "<img src=\"./img/adrian_yngve_1926_apparatus.gif\"/>\n",
    "\n",
    "Let us assume, as we often do with hypotheses that have won Nobel prizes, that this hypothesis is true. If we measure a higher rate in the foot after it is pressed and therefore claim that the alternative hypothesis is true, we have a *true positive* result. If, despite the veracity of the alternative hypothesis, we measure a lower firing rate in the stimulated foot (due to uncontrolled fluctuations in the firing rate, like those due to ion concentration changes, changes in ambient temperature, movement by the experimental subject, etc.), and therefore claim that the null hypothesis is true, then we have a *false negative result*. If the alternative hypothesis that stimulation of the foot with a weight increases firing rate were instead false, and we measured no difference in firing rate, or a difference in the opposite direction, then we would have a *true negative* result. Lastly, we could measure an increase even if the hypothesis were false, again due to uncontrolled factors, and the result would be a *false positive*.\n",
    "\n",
    "Note that if we had designed our test differently, we might expect the chances of different outcomes to change. For example, we are certainly free to design a really bad test, like measuring things the exact same way we did above, but claiming the opposite of what our experimental findings suggested. Note that this wouldn't always work to our deteriment: all of our false positives would become true negatives, for example. More sensibly, we might take ten measurements of each condition, then compute the averages before subtracting. Or we might phrase the null hypothesis differently, like \"the increase in firing rate is less than 10%\". We see intuitively that this is better design, but we don't know how much better. If we want to better understand the outcomes of our hypothesis tests, we need to be more rigorous about how we design our experiments, state our hypotheses, and make our decisions.\n",
    "\n",
    "#### Making Hypothesis Testing Rigorous\n",
    "\n",
    "At a high level, in statistical hypothesis testing, we take some data and use that to determine whether we claim $H_A$ or $H_0$. The key insight that lets us think rigorously about statistical hypothesis testing is to treat both the correct answer (columns in the above table) and the answer we give (rows in the above table) as uncertain quantities, as *random variables*.\n",
    "\n",
    "It may seem strange to think of our claims about the world as being random, since random is used colloquially to mean \"arbitrary\" or \"without structure or meaning\". But recall that anything we calculate from randomly-sampled data \"inherits\" some randomness from that data -- its value can be different each time we collect a dataset. Because our decisions are based on our data, they can be different from experiment to experiment. Put another way, the result of calculating something based on data is a statistic, and statistics have sampling distributions, as discussed in the earlier material.\n",
    "\n",
    "The \"correct answer\" also isn't random in the sense that most people think of randomness. However, it's also not random in the sense described above: a statement like \"this intervention has an effect\" is either true or false, and it doesn't change whether it's true or false depending on the data we collect. Instead, we recognize that we aren't entirely certain whether \"this intervention has an effect\" is true or not, and we instead write down a number that captures the degree to which we believe that the statement is true: a logically true statement like $2+2=4$ is associated with the number $1$, while a logically false statement like \"This number is greater than itself\" is associated with the number $0$. All other kinds of statements, like \"It will rain tomorrow\" or \"Extraterrestrial life exists\" fall somewhere in between. We call this number the probability that the statement is true.\n",
    "\n",
    "The view that randomness arises from sampling is a core component of the *frequentist* view of statistics. The view of that randomness arises from uncertainty is a core component of the *Bayesian* view of statistics. We'll avoid being dogmatic in this course, switching freely between the views whenever one or the other is simpler.\n",
    "\n",
    "### Joint Probabilities\n",
    "\n",
    "Now that we have two different random variables, the outcome of our testing procedure and the ground truth, we can think of the probability that any pair of events occurs, where the first element of a pair comes from the first random variable and the second element comes from the second random variable.\n",
    "\n",
    "If we shorten the events to $+$ and $-$ for the outcome and $T$ and $F$ for the correct answer (where $T$ means the alternative hypothesis is true), we write a table just like the one above to store the probabilities of pairs of events:\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td>&nbsp; $p(+,T)$ &nbsp;</td>\n",
    "      <td>&nbsp; $p(+,F)$ &nbsp;</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td>&nbsp; $p(-,T)$ &nbsp;</td>\n",
    "      <td>&nbsp; $p(-,F)$ &nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "where $p(+,T)$ should be read as \"the probability the test is positive and the alternative hypothesis is true\". Since these probabilities tell us the chance that two events *both occur*, we call them *joint probabilities*. The table above is called a *joint probability table*. The information it stores is called a *joint probability distribution*. In this case, the distribution is a mass function.\n",
    "\n",
    "A joint probability distribution is a powerful thing -- if we had access to all of the numbers in the joint probability tables for our experiments, it'd make statistical test design much easier! Unfortunately, constructing these tables can be very difficult and involves a degree of subjectivity. \n",
    "\n",
    "For now, let's assume a God's eye view, where we know all of these numbers, and see what we can figure out. We'll use a running example with the joint probability table below. Notice that these numbers add up to 1 -- that's what makes the values a valid probability distribution.\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td>$0.5$</td>\n",
    "      <td>$0.15$</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td>$0.05$</td>\n",
    "      <td>$0.3$</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Marginal Probabilities\n",
    "\n",
    "First, we can use the joint probability table to figure out the probabilities of the individual random variables that make up the table. For example, we can figure out the probability that the alternative hypothesis is true.\n",
    "\n",
    "We do this by simply adding up the probabilities of all events in which the alternative hypothesis is true. In this case, there are two such events: the alternative hypothesis is true and we claim it is true $(+,T)$ and the alternative hypothesis is true and we claim it is false $(-,T)$. These events correspond to the cells in the first column. Similarly, we can calculate the probability that the alternative hypothesis is false by adding up the values in the second column or calculate the probability of each outcome of our test by adding up the appropriate row.\n",
    "\n",
    "Below, you'll find these values are worked for our example table. There, as traditionally, the probability of an event is written at the end of the column or row corresponding to that event. These areas are called the *margins* and so these probabilities are called *marginal probabilities*. Notice that if we add up the marginal probabilities along a row or column (the numbers with a particular background color), the result is 1. That means these are probability distributions -- the marginal probability distributions of the test outcome and the alternative hypothesis.\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "      <th style=\"background-color: rgb(255,204,204);\"> &nbsp; $p(\\text{test})$ &nbsp; </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td>$0.5$</td>\n",
    "      <td>$0.15$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">$0.65$</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td>$0.05$</td>\n",
    "      <td>$0.3$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">$0.35$</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> &nbsp; $p(H_A)$ &nbsp; </td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">$0.55$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">$0.45$</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "There's nothing magical happening here. We could've made the rows correspond to a different random variable, like whether it's raining in Kansas or whether Mercury is in retrograde. After all, the total chance that the alternative hypothesis is true is equal to the chance that it is equal and Mercury is in retrograde plus the chance that it is equal and Mercury is not in retrograde (it might be helpful to think in terms of frequencies here). Calculating marginal probabilities from a joint probability table is just an accounting trick to make more obvious the information that's already in the table.\n",
    "\n",
    "\n",
    "\n",
    "The marginal probability distribution of the alternative hypothesis is of particular importance. It tells us what we believe about the world *before* we take into account the result of our statistical test. For this reason, it's called the *prior* probability of the hypothesis. There is some degree of subjectivity in setting prior probabilities, since they arise from complex, fuzzy factors related to our fundamental beliefs about the world (are our educated guesses usually right or usually wrong? is nature usually simple or usually messy?) and the combined results in the literature.\n",
    "\n",
    "### Conditional Probabilities\n",
    "\n",
    "The joint probability table told us the chance that any particular pair of events occurs, while the marginal probability distributions told us the chance that any individual event occurs, irrespective of the other variable in the pair.\n",
    "\n",
    "Often, however, we know the value of one of the variables. For example, once we've run the statistical test, we know what the outcome turned out to be. Other times, we would like to assume that one of the variables takes on a particular value (e.g. that the null hypothesis is true) and determine how the probability of each outcome for the other variable has changed. Do we need to throw out our old, possibly hard-won, joint probability table and start over?\n",
    "\n",
    "Luckily, the answer is no. Using the joint probability table, we can construct two new tables, which tell us the probability distribution of one of the random variables for fixed values of the other. Because the probabilities in these tables only pertain when a certain *condition* is satisfied (these probabilities are *conditional* on the other random variable having a certain value), they are called *conditional probability tables*.\n",
    "\n",
    "How do we determine the values in these tables? Consider the right column of the joint probability table above, corresponding to all cases where the alternative hypothesis is false. This column *almost* tells us the conditional probabilities. For example, we can readily see that when the alternative hypothesis is false, the probability that the test comes up negative is twice the probability that it comes up positive ($0.3\\ =\\ 2\\cdot0.15$).\n",
    "\n",
    "However, $0.3 + 0.15$ doesn't equal $1$ -- it's equal to $0.45$, so we can't just directly use those numbers for the conditional probabilities. However, if we divide them by their sum, they'll add up to $1$:\n",
    "\n",
    "$$\n",
    "\\frac{0.3}{0.3+0.15} + \\frac{0.15}{0.3+0.15} = \\frac{0.3+0.15}{0.3+0.15} = 1\n",
    "$$\n",
    "\n",
    "Put another way, the rows and columns of the joint probability table are like *un-normalized* conditional distributions -- distributions that don't add up to one. To make them into proper distributions, we need to normalize them by dividing by their sums, which happen to correspond to the marginal probabilities.\n",
    "\n",
    "The two conditional probability tables for our running example appear below. One corresponds to viewing the rows of the joint table as un-normalized distributions (and so conditioning on test outcome, the random variable in the rows of the table) while the other corresponds to viewing the columns as un-normalized distributions (and so conditioning on the truth value of the alternative hypothesis, the random variable in the columns of the table).\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">0.77</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">0.23</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">$p(H_A\\lvert +)$</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">0.14</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">0.86</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">$p(H_A\\lvert -)$</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">0.91</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">0.33</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">0.09</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">0.67</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td ></td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(\\text{test}\\lvert T)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(\\text{test}\\lvert F)$&nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "The vertical bar, $\\vert$, is pronounced \"conditioned on\" or \"given\". One would read the expression $p(\\text{test}\\vert T)$ as \"the conditional probability distribution of the test outcome given that the alternative hypothesis is true\".\n",
    "\n",
    "Note one important difference between a conditional probability table and a joint probability table: while the latter is a distribution, the former is NOT. For example, the entries of a conditional probability table don't add up to 1. Instead, each row or column of a conditional probability table adds up to 1. A conditional probability table is a collection of distributions, with one distribution for each value of the variable being conditioned on.\n",
    "\n",
    "Because of this distinction, there are several entities that end up getting called \"the conditional probability\". For example, the first table above is \"the conditional probability of the alernative hypothesis given the test outcome\". The first row in that table is \"the conditional probability of the alternative hypothesis given that the test is positive\". The first cell in that row is \"the conditional probability that the alternative hypothesis is true given that the test is positive\".\n",
    "\n",
    "In English, this distinction is clear enough, but unfortunately the usual mathematical notation for all three of the above is $p(x\\vert y)$, with the meaning depending on which of $x$, $y$, or both are outcomes (e.g. \"test is positive\") and which are random variables (e.g. \"the outcome of the test\").\n",
    "\n",
    "### Conditional Probabilities and Hypothesis Testing\n",
    "\n",
    "We introduced joint probabilities in order to understand hypothesis testing. Now that we are armed with the two conditional probability tables associated with the joint probability table, we can start to dive deeper. Let's start with the row-wise conditional probability distributions.\n",
    "\n",
    "#### The \"Test-Interpretation\" Table\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(T\\lvert +)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(F\\lvert +)$&nbsp;</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(T\\lvert -)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(F\\lvert -)$&nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Because these probabilities are conditioned on the outcome of a test, they tell us how to interpret the results of a test that we have performed.\n",
    "\n",
    "Consider the top row of this table. This row tells us the conditional probability of the alternative hypothesis when we've gotten a positive test result. Notice that $p(T\\vert +)$ isn't $1$ -- a positive test result doesn't mean that we are now 100% certain that that the alternative hypothesis is true. In the case of our running example, it's $0.77$.\n",
    "\n",
    "Recall that we already had a number that reflected our belief that the alternative hypothesis is true: the prior probability of the alternative hypothesis. The conditional probability table above tells us how we should update that belief when we see the result of the test. Since these probabilities come after we collect data and perform a test, they are called *posterior* probabilities.\n",
    "\n",
    "Posterior and conditional probabilities are very general concepts. Because of the importance of binary hypothesis testing, the conditional probabilities in the table above have special names that capture their role in interpreting tests. Those names are:\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">Positive Predictive Value</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">False Discovery Rate</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">False Omission Rate</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">Negative Predictive Value</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "where coloring, as above, indicates conditioning on the same value. Because of this, two named quantities with the same background color must add up to 1, and so knowing one tells you the other. Depending on the context of the problem, one will be easier or harder to think about.\n",
    "\n",
    "The *positive predictive value* was described above. The term *false discovery rate* arises from considering what would happen if we ran a particular test many times. The false discovery rate tells us the fraction of our positives that would be false positives (or false *discoveries*, when the alternative hypothesis being true means the discovery of a new phenomenon or drug).\n",
    "\n",
    "The values in the second row mirror the values in the first row. The *negative predictive value* tells us the posterior probability that a negative test result reflects the truth -- higher values mean that negative results on the test are more meaningful. The *false omission rate* is akin to the false discovery rate, but it tells us the fraction of our negatives that are false negatives, or incorrect omissions of certain phenomena or candidate drugs from our list of real or effective ones.\n",
    "\n",
    "Next, let's consider the column-wise conditional probability distributions.\n",
    "\n",
    "#### The \"Test-Design\" Table\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(+\\lvert T)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(+\\lvert F)$&nbsp;</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(-\\lvert T)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(-\\lvert F)$&nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Because these probabilities are conditioned on whether the alternative hypothesis is true or false, they don't tell us how to interpret the results of a test. Instead, they tell us how the test will perform in situations where the hypothesis is true and where it is false.  Notice that the diagonal values correspond to the probability that the test gives you the right answer in each case. These probabilities are useful for folks who design statistical tests: without having to worry about whether the alternative hypothesis is likely to be true or false, they can confirm that their test is useful by ensuring that the diagonal elements of the above table are both large.\n",
    "\n",
    "As above, these quantities have special names to distinguish them from run-of-the-mill conditional probabilities. Because they are more commonly used and used by a wide array of disciplines, they have multiple names, the most common of which appear below.\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> True Positive Rate, Power, Sensitivity </td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\"> False Positive Rate, $\\alpha$ </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> False Negative Rate, $\\beta$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\"> True Negative Rate, Specificity</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Take care when interpreting the terms that end in rate, like \"true positive rate\" and \"false negative rate\". The temptation is to interpret them as referring to the fraction of your tests that are true positives/false negatives. This is incorrect. Instead, these rates tell you the fractions of such tests *in situations where the alternative hypothesis is true*. That is, if we know the alternative hypothesis is true, then we can use the true positive rate to tell us how many of our tests should be true positives. The false positive rate and true negative rate tell us the false positive and true negative fractions *in situations where the null hypothesis is true*.\n",
    "\n",
    "At first, these numbers seem to be of limited use for scientists, since we certainly wouldn't be doing experiments if we knew whether the hypothesis was true or false!\n",
    "\n",
    "The utility of this table is that it doesn't require us to specify $p(H_A)$, so we can avoid all of the difficulties described above for figuring out prior probabilities. The rightmost column of this table is particularly easy to calculate, so it has long dominated the design of hypothesis tests. To make things a bit more concrete, we now turn to how we calculate the values in this column and use them to design a test.\n",
    "\n",
    "### Good: Hypothesis Test Design Using $p(+\\lvert F)$\n",
    "\n",
    "#### Example: Adrian and Yngve\n",
    "\n",
    "Let us return to our neuroscience example: we wish to test the hypothesis that pressing on a cat's hind foot causes an increase in the firing rate of the nerve exiting the foot.\n",
    "\n",
    "Imagine that we knew the probability distribution of firing rates that we measure in the case where the foot is not being stimulated. For example, we might have collected thousands of trials, so we have a very good estimate of this probability distribution. Given a new measurement value, we might like to know how likely it was that we'd get that value. Unfortunately, the answer to that question for a continuous value like firing rate is always $0$, as discussed in the earlier section on probability density functions. Instead, we have to ask how likely it was that we'd measure a firing rate around that value, or at least as large or small as that value. In both cases, we take an area under the curve given by the probability density function.\n",
    "\n",
    "A key insight that lets us rigorously design a statistical test here is that if the null hypothesis is true, then the distribution we measured for the firing rates in the nerve of the unstimulated foot is exactly the distribution of the firing rates in the stimulated foot, *under the null hypothesis*. This distribution is often called the *null distribution*. Armed with this distribution, we can calculate the chance that we would observe a firing rate at least that large if the null hypothesis were true. It's simply the procedure described above: take the probability distribution given by the observations of the unstimulated foot and measure the area under the curve that is at or above the measured value. \n",
    "\n",
    "The resulting number is the chance, *under the null hypothesis*, that we would collect the data that we collected.   This is a famous number. It is called the *$p$-value*. The lower this value is, the less likely it is that we would observe the data that we observed under the null hypothesis. Put another way, if this value is low and the null hypothesis is true, then our results are very surprising -- they represent some sort of cosmic coincidence. Science, and much of rational inference, rests on the assumption that being less surprised is better and bizarre coincidences don't occur that often. All else being equal, if another hypothesis results in us being less surprised by the information we have, then we prefer that hypothesis.\n",
    "\n",
    "We can use the $p$-value to design a statistical test whose false positive rate we know exactly. If we reject the null hypothesis and claim that the alternative hypothesis is true whenever the $p$-value is less than some number $\\alpha$, then our false positive rate will be $\\alpha$. It's helpful to spell out exactly why. Remember that the $p$-value is the chance that we would observe the data we observed under the null hypothesis. By definition, half of time, we'll measure $p$-values that are under $0.5$ -- in this case, we can be explicit and say that half of the data we observe should be at least as large as the median. This same argument holds for each percentile.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Let's abstract a bit from the example given above.\n",
    "\n",
    "The general principle that we used to determine whether a test was a good choice or not was that if an event occurs that is too unlikely to occur under a given hypothesis, then we discard that hypothesis. The event we focus on is the value of a statistic, a function of our data, which we call the *test statistic*. Good test statistics usually have two properties: \n",
    "\n",
    "1. We know their sampling distribution under the null hypothesis\n",
    "2. That distribution has a single peak, with values falling off quickly on either side\n",
    "\n",
    "The second property lets us say that the event we're interested in determining the likelihood of is *observing an extreme value of the test statistic*, where extreme usually means far away, in either direction, from the peak of the sampling distribution, but could mean far away in a particular direction. The first property lets us figure out what the likelihood of that extreme event is. This likelihood is called the $p$-value (or, more accurately, $p$-statistic). Once we've done that, we compare our $p$-value to some pre-selected value $\\alpha$ (the common, but my no means compulsory or necessary, choice is $0.05$), and if the observed value is lower, we reject the null hypothesis.\n",
    "\n",
    "If we're not interested in determining the $p$-value exactly, but just want to know the outcome of our test, we can use the sampling distribution of the test statistic under the null hypothesis (also known as the null distribution of the test statistic, for \"short\") to calculate the value of the test statistic that will be extreme enough to be just under our pre-selected value. We call this value the *critical value* of the test statistic.\n",
    "\n",
    "The only difference between the simple test we described above and even very complex statistical tests is that our choice of statistic was very simple: the value of a single observation. For most modern experiments, this test is not good enough -- for example, the chance of a true positive might be almost 0! Let's improve our design process for statistical tests by looking at some of the other entries in our conditional probability tables.\n",
    "\n",
    "### Better: Hypothesis Test Design Using $p(-\\lvert T)$\n",
    "\n",
    "Because both columns of our \"test-design\" conditional probability table must add up to $1$, there are really only two independent entities. Above, we covered one of those, the *false positive rate* and its twin, the *true negative rate*. \n",
    "\n",
    "The other pair is the *false negative rate* and its twin, the *true positive rate*. The true positive rate is also known as the *power*, so this section might also be titled \"Power Analysis\". For reference, we reproduce the conditional probability table here:\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> True Positive Rate, Power, Sensitivity </td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\"> False Positive Rate, $\\alpha$ </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> False Negative Rate, $\\beta$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\"> True Negative Rate, Specificity</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "The critical difference between this column (on the left) and the column considered above (on the right in the table) is that, where previously the event being conditioned on was that the *null hypothesis* was true, now, the event being conditioned on is that the *alternative hypothesis* is true.\n",
    "\n",
    "Just as before, if we want to calculate the numbers in this column, we need to know the distribution of our test statistic under the hypothesis we're assuming is true. Unfortunately, this is much more complicated in the case of alternative hypotheses than for null hypotheses. One cause of this is that the null hypothesis is very specific: \"there is no effect of this intervention on the measured variable\". Another way to phrase this, using more technical language, might be \"the size of the effect of this intervention is $0$\". The alternative hypothesis is often less specific.\n",
    "\n",
    "Consider our running example: the null hypothesis was that there was no change in the firing rate between nerves coming from stimulated and unstimulated cats' feet. That is, the change in firing rate caused by stimulating the foot with a weight was $0$. The alternative hypothesis was not so specific: we hypothesized that there *was* a change, but we didn't say how big it was. That is, we didn't specify an *effect size*.\n",
    "\n",
    "This makes it difficult to figure out how likely we are to make a false negative error. If the effect of stimulation on firing rate is tremendously large, then the chance of a false negative is lower -- it'd take a very large coincidence of uncontrolled factors to \"cancel it out\" and make us erroneously conclude that the null hypothesis was true. If the firing rate change is extremely small, then a false negative is extremely likely -- we need the uncontrolled factors to coincidentally break in the same direction as the intervention in order to reject the null.\n",
    "\n",
    "This underscores the importance, however, of calculating a true positive rate. Science experiments are expensive and time-consuming, and it would be wasteful to perform experiments for which there is little hope of getting the correct result. For a sobering account of the issues caused by the widespread failure to account for the true positive rate, check out\n",
    "[Button, Ioannidis, et al., 2013](http://www.nature.com/nrn/journal/v14/n5/abs/nrn3475.html).\n",
    "\n",
    "In order to calculate the true positive rate, we need to specify the distribution under the alternative hypothesis, which usually means specifying an exact value for the size of the effect that we believe we'll find. This raises two issues. First, we need to determine how we specify that value. Second, we need to calculate the true positive or false negative rate from that value.\n",
    "\n",
    "The solution to the first problem is straightforward, if rarely done outside of certain contexts like large clinical trials or very expensive, long-running experiments. We run a very small experiment, also known as a pilot, and calculate an effect size from that experiment. We don't care whether the effect was significant or not; we simply use it as an estimate of the true effect size. This gives us a specific alternative hypothesis to test and so gives us, in principle, a sampling distribution of the test statistic under the alternative hypothesis.\n",
    "\n",
    "The second problem is a bit tougher. Though pre-written methods for calculating the distribution of many test statistics under various null hypotheses are widely available, methods for calculating these distributions under alternative hypotheses are harder to come by. Firstly, there is less demand, since the scientific community has historically been less aware of the importance of calculating true positive rates. Awareness has spread, but roll-out of this kind of *power analysis* software is limited. For example, the availability of such packages for Python is quite limited. Secondly, calculating the sampling distribution for a specific value of the effect is often quite involved, often more so than for the absence of an effect. The difficulty is increased if, as is often desirable, we want to say \"the effect is no bigger than $a$ and no less than $b$\".\n",
    "\n",
    "### Best: Hypothesis Test Design Using $p(T\\lvert +)$ and $p(F\\lvert -)$\n",
    "\n",
    "In an ideal world, scientific experiments would be designed to maximize the probability that inferences drawn based on the results are correct: to maximize the chance that the hypothesis supported by the results is true. The two relevant quantities for this kind of test design are $p(T\\lvert+)$, the posterior probability that the alternative hypothesis is true given that we observe a positive test result, and $p(F\\lvert-)$, the posterior probability that the alternative hypothesis is false given that we observe a negative test result. These two quantities are found in the \"test-interpretation\" conditional probability table, reproduced below.\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(T\\lvert +)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(F\\lvert +)$&nbsp;</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(T\\lvert -)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(F\\lvert -)$&nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">Positive Predictive Value</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">False Discovery Rate</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">False Omission Rate</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">Negative Predictive Value</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Unfortunately, these require an estimate of the prior probability of the hypothesis. Adequately representing our prior beliefs quantitatively is an inexact science, so there will usually be room for subjectivity -- often enough to completely change the interpretation of results.\n",
    "\n",
    "In some cases, however, these quantities can be estimated from data: see \n",
    "[this *Points of Significance* article](http://www.nature.com/nmeth/journal/v11/n4/full/nmeth.2900.html)\n",
    "for an introduction.\n",
    "These methods require, however, that we test a large number of roughly equivalent hypotheses. For example, we measure the levels of expression for thousands of genes in healthy and unhealthy cells, with the collection of equivalent hypotheses being \"expression of Gene 1 is different in unhealthy cells\", \"expression of Gene 2 is different in unhealthy cells\", and so on.\n",
    "\n",
    "### Aside: $p(F\\lvert +)$ is not $p(+\\lvert F)$\n",
    "\n",
    "The \"test-design\" conditional probability tables from above are reproduced below.\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> True Positive Rate, Power, Sensitivity </td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\"> False Positive Rate, $\\alpha$ </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\"> False Negative Rate, $\\beta$</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\"> True Negative Rate, Specificity</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > $T$ </th>\n",
    "      <th > $F$ </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >$+$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(+\\lvert T)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(+\\lvert F)$&nbsp;</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >$-$</td>\n",
    "      <td style=\"background-color: rgb(204,204,255);\">&nbsp;$p(-\\lvert T)$&nbsp;</td>\n",
    "      <td style=\"background-color: rgb(255,204,204);\">&nbsp;$p(-\\lvert F)$&nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Notice the position of $\\alpha$. $\\alpha$ is not the chance that the null hypothesis is true, given the results of the test. That would be $p(F\\lvert +)$, from the \"test-interpretation\" conditional probability table. Instead, $\\alpha$ is $p(+\\lvert F)$, which is the probability of getting a positive result, given that the null hypothesis is false.\n",
    "\n",
    "A silly example might be helpful for understanding and remembering the difference. Say you wanted to test the hypothesis that $2+2=5$. We know this hypothesis is false, because we've defined it to be false. Now imagine that, to test this hypothesis, you flip a coin. If it comes up heads, then you claim $2+2=5$ is true, and if it comes up tails, you claim $2+2=5$ is false. This is a pefectly acceptable, if useless, statistical test. The chance of getting a false positive is not $0$ -- it's $0.5$, since the odds are $50/50$. Therefore, $p(+\\lvert F)$ is $0.5$. However, it's intuitively obvious that $p(F\\lvert +)$ is still $1$ -- no amount of coin flipping can change the fact that $2+2\\neq5$ -- and indeed, if you work through the calculations described above, you can confirm this. To get you started, I've provided the joint probability table below. \n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th > &nbsp;&nbsp; $2+2=5$ &nbsp;&nbsp;</th>\n",
    "      <th > &nbsp;&nbsp; $2+2\\neq5$ &nbsp;&nbsp; </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td >Tails</td>\n",
    "      <td>$0$</td>\n",
    "      <td>$0.5$</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td >Heads</td>\n",
    "      <td>$0$</td>\n",
    "      <td>$0.5$</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "It's important to keep this in mind when interpreting $p$-values. They are frequently confused for the probability that the hypothesis is incorrect, more technically the posterior probability of the null hypothesis. It's important to recall that a very small $p$-value doesn't necessarily mean a very high posterior probability of the hypothesis, in particular if the prior probability of the hypothesis is small. For a polemic presentation of this view, see\n",
    "[Colquhoun, 2014](http://rsos.royalsocietypublishing.org/content/1/3/140216)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "neur299",
   "language": "python",
   "name": "neur299"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
